{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTVQU_CNQiM7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = input(\"Kata Kunci: \")\n",
        "total_pages = input(\"Jumlah Artikel [10~100~1000]: \")\n",
        "articles = []\n",
        "\n",
        "for page in range(0, int(total_pages)):\n",
        "    url = f\"https://search.okezone.com/searchsphinx/loaddata/article/{keyword}/{page}\"\n",
        "    \n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, \"html5lib\")\n",
        "\n",
        "    article_scrape = soup.find(\"div\", class_=\"listnews\")\n",
        "\n",
        "    published_time = article_scrape.find(\"div\", class_=\"tgl\").get_text()\n",
        "    title = article_scrape.find(\"div\", class_=\"title\").get_text()\n",
        "    href = article_scrape.a[\"href\"]\n",
        "\n",
        "    page_more = requests.get(href)\n",
        "    soup_more = BeautifulSoup(page_more.text, \"html5lib\")\n",
        "\n",
        "    content = \"\"\n",
        "    if soup_more.find(\"div\", {\"id\": \"contentx\", \"class\": \"read\"}):\n",
        "      paragraphs = soup_more.find(\"div\", {\"id\": \"contentx\", \"class\": \"read\"}).find_all(\"p\")\n",
        "      for p in paragraphs:\n",
        "        paragraph = p.get_text(strip=True)\n",
        "        if not paragraph.endswith(\"*\"):\n",
        "            content += paragraph\n",
        "\n",
        "    journalis = list(soup_more.find(\"div\", class_=\"namerep\").stripped_strings)[0].strip(\",\")\n",
        "        \n",
        "    articles.append({\n",
        "            \"published_time\": published_time,\n",
        "            \"title\": title,\n",
        "            \"content\": content,\n",
        "            \"journalis\": journalis,\n",
        "            \"url\": href\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(articles)\n",
        "time_scrape = datetime.now().strftime(\"%m%d%Y_%H%M%S\")\n",
        "df.to_csv(f\"result_by-query_{keyword}_{time_scrape}.csv\", index=False)"
      ],
      "metadata": {
        "id": "_mtejrbLDyun"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Skripsi - Crawling Data Okezone",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}